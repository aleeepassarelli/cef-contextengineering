"""
core/context_metrics.py
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
M√≥dulo de m√©tricas fundamentais do Context Engineering Framework (CEF).

Fornece fun√ß√µes para medir:
- Densidade Sem√¢ntica (SD)
- Press√£o Contextual (PC)
- Entropia Sem√¢ntica (S_H)
- Coer√™ncia Lexical (Œº)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Autor: Context Engineering Lab
Licen√ßa: MIT
Vers√£o: 1.0.0
"""

import math
import re
from collections import Counter
from typing import Dict, List


# ============================================================
# üîπ FUN√á√ÉO: calcular densidade sem√¢ntica (SD)
# ============================================================

def calculate_sd(text: str) -> float:
    """
    Calcula a Densidade Sem√¢ntica (SD) de um texto.
    
    SD = (vocabul√°rio √∫nico / total de tokens) * fator de coer√™ncia
    
    Onde:
      - Fator de coer√™ncia ‚âà propor√ß√£o de termos significativos
      - SD varia entre 0.0 e 1.0
    """
    if not text or not isinstance(text, str):
        return 0.0

    tokens = re.findall(r"\b\w+\b", text.lower())
    if len(tokens) == 0:
        return 0.0

    unique = set(tokens)
    ratio = len(unique) / len(tokens)

    # Pondera√ß√£o: reduz impacto de textos muito curtos
    coherence_factor = 1 - math.exp(-len(tokens) / 50)
    sd = min(1.0, ratio * coherence_factor)

    return round(sd, 4)


# ============================================================
# üîπ FUN√á√ÉO: calcular entropia sem√¢ntica (S_H)
# ============================================================

def semantic_entropy(text: str) -> float:
    """
    Mede a entropia sem√¢ntica (S_H), ou dispers√£o lexical do texto.
    
    Base: Shannon Entropy aplicada ao vocabul√°rio.
    """
    tokens = re.findall(r"\b\w+\b", text.lower())
    if len(tokens) <= 1:
        return 0.0

    counts = Counter(tokens)
    total = sum(counts.values())

    entropy = -sum((c / total) * math.log2(c / total) for c in counts.values())
    normalized = entropy / math.log2(len(tokens))
    return round(normalized, 4)


# ============================================================
# üîπ FUN√á√ÉO: coer√™ncia lexical (Œº)
# ============================================================

def lexical_coherence(text: str) -> float:
    """
    Mede a coer√™ncia lexical (Œº) ‚Äî regularidade sem√¢ntica e repeti√ß√£o √∫til.
    Œº = 1 - S_H (com ajuste para densidade)
    """
    sd = calculate_sd(text)
    sh = semantic_entropy(text)
    mu = (1 - sh) * sd
    return round(mu, 4)


# ============================================================
# üîπ FUN√á√ÉO: densidade de contexto (CD)
# ============================================================

def context_density(components: Dict[str, str]) -> float:
    """
    Calcula a densidade m√©dia ponderada do contexto total.
    
    Pesos padr√£o:
      system: 0.30
      user: 0.25
      history: 0.15
      rag: 0.20
      tools: 0.10
    """
    weights = {
        "system": 0.30,
        "user": 0.25,
        "history": 0.15,
        "rag": 0.20,
        "tools": 0.10,
    }

    total = 0.0
    for k, w in weights.items():
        if k in components:
            total += calculate_sd(components[k]) * w

    return round(total, 4)


# ============================================================
# üîπ FUN√á√ÉO: press√£o contextual (PC)
# ============================================================

def contextual_pressure(context: Dict[str, any]) -> float:
    """
    Mede a Press√£o Contextual (PC), que expressa a satura√ß√£o sem√¢ntica.
    
    PC = CD * (len(tokens) / 10_000)
    
    Classifica√ß√£o:
      0.0‚Äì0.4 ‚Üí raso (incompleto)
      0.4‚Äì0.7 ‚Üí racional (minimalismo)
      0.7‚Äì0.9 ‚Üí criativo (satura√ß√£o)
      >0.9   ‚Üí entr√≥pico (alucina√ß√£o)
    """
    tokens = []
    for v in context.values():
        if isinstance(v, str):
            tokens += re.findall(r"\b\w+\b", v.lower())

    total_tokens = len(tokens)
    cd = context_density(context)

    if total_tokens == 0:
        return 0.0

    pc = cd * (total_tokens / 10_000)
    return round(min(pc, 1.0), 4)


# ============================================================
# üîπ FUN√á√ÉO: regime contextual
# ============================================================

def classify_context_regime(sd: float, pc: float) -> str:
    """
    Classifica o regime de opera√ß√£o contextual com base em SD e PC.
    """
    if sd < 0.7:
        return "Incompleto"
    if pc < 0.4:
        return "Raso"
    if 0.4 <= pc < 0.7:
        return "Minimalismo"
    if 0.7 <= pc < 0.9:
        return "Satura√ß√£o"
    if 0.6 <= pc <= 0.8 and 0.7 <= sd <= 0.85:
        return "Equil√≠brio"
    return "Entr√≥pico"


# ============================================================
# üîπ TESTE R√ÅPIDO
# ============================================================

if __name__ == "__main__":
    sample = {
        "system": "Agente anal√≠tico especializado em s√≠ntese cognitiva e compress√£o sem√¢ntica.",
        "user": "Explique a diferen√ßa entre contexto simb√≥lico e contexto l√≥gico.",
        "history": "√öltima sess√£o abordou modelos de coer√™ncia e entropia lingu√≠stica.",
        "rag": "Fonte: 'Poetics of Systems', 2024.",
        "tools": "semantic_search, code_runner"
    }

    sd = context_density(sample)
    pc = contextual_pressure(sample)
    regime = classify_context_regime(sd, pc)

    print(f"SD = {sd} | PC = {pc} ‚Üí Regime: {regime}")
